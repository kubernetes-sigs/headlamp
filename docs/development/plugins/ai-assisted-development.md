---
title: AI-Assisted Development
sidebar_position: 7
---

# AI-Assisted Plugin Development

This guide explains how to leverage the pre-generated context file (`headlamp-plugins-context.txt`) to accelerate Headlamp plugin development using AI assistants like GitHub Copilot Chat, Cursor, or other large language models (LLMs).

## The Plugin Context File

The repository contains a file named `headlamp-plugins-context.txt` located at the root.

This file is automatically generated by running:

```bash
make llms-context
```

*(Requires `uv pip install gitingest`)*

The `make` command uses the `gitingest` tool to gather relevant information from key plugin development documentation, the core plugin API source code (`plugins/headlamp-plugin/`), and the example plugins (`plugins/examples/`). It concatenates this information into the single `headlamp-plugins-context.txt` file.

**Note:** This file currently contains concatenated, unstructured text digests. While highly informative, it doesn't yet follow the structured format specified by the llms.txt standard (e.g., `llms-ctx-full.txt`). The goal is to align with this standard in the future as tooling evolves.

## Using the Context with AI Assistants

The primary benefit of this context file is providing your AI assistant with specific, relevant information about Headlamp plugin development, significantly improving the quality and accuracy of its suggestions and code generation.

Here's the typical workflow:

1.  **Initialize Plugin (if needed):** If you haven't already, create your basic plugin structure using the Headlamp plugin tool:
    ```bash
    npx --yes @kinvolk/headlamp-plugin create <your-plugin-name>
    ```
2.  **Locate the Context File:** Find `headlamp-plugins-context.txt` in the root of the Headlamp repository.
3.  **Load into Context:** Copy the *entire contents* of this file and paste it into the chat interface or context-loading mechanism of your AI assistant.
    *   **Cursor:** You can often directly add files to the chat context using `@` symbol or specific commands.
    *   **Other Chat Interfaces:** Paste the content directly into the chat prompt, usually preceding your specific request.
4.  **Provide Instructions:** Once the context is loaded, ask the AI assistant to help you with plugin development tasks. Be specific! See the examples below.
5.  **Verify Generated Code:** After the AI generates code, use the built-in tools to check and format it:
    ```bash
    # Navigate into your plugin directory first
    cd <your-plugin-name>
   npm run format
   npm run lint
    ```

## Example Prompts

Here are some examples of how you might prompt an AI assistant after loading the `headlamp-plugins-context.txt`:

*   "Using the provided context about Headlamp plugin development, help me create a basic plugin skeleton named 'my-checker'. Initialize it correctly according to the documentation and examples."
*   "Based on the context, show me how to add a new item to the cluster sidebar menu in my Headlamp plugin."
*   "Explain how Headlamp plugins register custom routes, using examples from the context."
*   "Generate the code for a simple Headlamp plugin component that fetches all Pods in the 'default' namespace and displays their names in a list, following the patterns in the context."
*   "Check the following plugin code snippet for correctness based on the Headlamp plugin API described in the context: [Your Code Snippet]"

## Benefits

By providing this curated context, you enable the AI to:

*   Understand Headlamp's specific plugin structure and APIs.
*   Refer to actual code examples from the repository.
*   Generate more accurate and idiomatic plugin code.
*   Answer questions about plugin development more effectively.

This significantly speeds up the learning curve and development process, especially for those new to Headlamp plugin development. 